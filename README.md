# ğŸ¤– Machine Learning Projects

Welcome to my collection of machine learning assignments, developed during the course of my ML studies.  
Each assignment explores different machine learning techniques â€” from unsupervised learning to advanced ensemble methods â€” with clear implementations and real-world data challenges.

---

## ğŸ“‚ Assignments Overview

### ğŸ“ Assignment 1: Clustering & Dimensionality Reduction (Unsupervised Learning)

**Objective**: Discover natural groupings in data without labels.

**Whatâ€™s Inside**:
- ğŸ§ª **K-Means Clustering** â€“ applied with multiple `k` values
- ğŸ“ **Principal Component Analysis (PCA)** â€“ used to reduce dimensionality for better visualization
- ğŸ“Š **Silhouette Score** â€“ used to evaluate clustering quality
- ğŸ—‚ï¸ Dataset: Generated synthetic datasets for clustering (2D, 3D)

ğŸ“Œ Youâ€™ll learn how to:
- Choose the number of clusters (`k`)
- Visualize clusters in 2D and 3D
- Interpret PCA-transformed data

---

### ğŸ“ Assignment 2: Supervised Learning â€“ Linear Regression, Naive Bayes & Decision Trees

**Objective**: Train models to predict numeric and categorical outcomes using labeled data.

**Key Components**:
- ğŸ“ˆ **Linear Regression**: Applied to predict continuous values with performance metrics like MSE and RÂ²
- ğŸ“Š **Naive Bayes**: Used for classification with text features (TF-IDF preprocessing included)
- ğŸŒ³ **Decision Trees**: Built and visualized for both regression and classification tasks

**Extras**:
- Feature encoding (LabelEncoder, TF-IDF)
- Train-test split, cross-validation
- Evaluation metrics: accuracy, confusion matrix, precision, recall

ğŸ“Œ Great for understanding:
- Difference between regression & classification
- Model evaluation strategies
- Handling both numeric and text data

---

### ğŸ“ Assignment 3: Data Preprocessing & AdaBoost (Ensemble Learning)

**Objective**: Prepare messy data and improve predictions using ensemble methods.

**Steps Taken**:
- ğŸ§¹ **Data Cleaning**:
  - Null values handled via:
    - KNN imputation for numerical features
    - Mode imputation for categorical features
- ğŸ—ï¸ **Feature Encoding & Scaling** (OneHotEncoder + StandardScaler)
- ğŸš€ **AdaBoost**:
  - Implemented **from scratch**
  - Used decision stumps as weak learners
  - Evaluated performance across boosting iterations

ğŸ“Œ Key Takeaways:
- How boosting improves model performance
- How to write your own ensemble algorithm
- Importance of good preprocessing in real-world data

---

## ğŸ§  Tools & Libraries Used

- **Python 3.x**
- **Jupyter Notebook**
- **scikit-learn** â€“ Models, metrics, preprocessing
- **Pandas / NumPy** â€“ Data manipulation
- **Matplotlib / Seaborn** â€“ Visualizations

---

## ğŸš€ Getting Started

### ğŸ“¦ Requirements

Install the required libraries:

```bash
pip install numpy pandas scikit-learn matplotlib seaborn
